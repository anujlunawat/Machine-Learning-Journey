{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4acb196-a6f3-4e0a-8451-60bd3c83db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df4aa2e-830a-444a-972a-0ed87e2b2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([\n",
    "    [2, 5, 3, 7, 15],\n",
    "    [4, 6, 2, 8, 20],\n",
    "    [5, 7, 4, 6, 22],\n",
    "    [3, 8, 5, 5, 21],\n",
    "    [6, 5, 6, 7, 24],\n",
    "    [7, 3, 5, 8, 23],\n",
    "    [8, 4, 7, 6, 25],\n",
    "    [5, 6, 4, 9, 24],\n",
    "    [6, 7, 3, 5, 21],\n",
    "    [4, 5, 6, 7, 22]\n",
    "], columns=['X1', 'X2', 'X3', 'X4', 'Y'])\n",
    "\n",
    "data = data.to_numpy()\n",
    "X = data[:, :4].T\n",
    "y = data[:, [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab308e5-575d-4a7a-be5b-dce3393f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for layer in range(1, L):\n",
    "        # the cols are weights associated with a node of current layer\n",
    "        # the rows are weights assoc with the node of previous layer\n",
    "        parameters[\"W\" + str(layer)] = np.random.rand(layer_dims[layer-1], layer_dims[layer]) - .5\n",
    "\n",
    "        parameters[\"b\" + str(layer)] = np.random.rand(1, layer_dims[layer]) -.5\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b533087-a13a-4db0-af8e-44c91fba28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    return np.dot(A_prev, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab70ae4-9cee-4976-a7bb-2744d30c2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for layer in range(1, L+1):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(layer)]\n",
    "        b = parameters['b' + str(layer)]\n",
    "        \n",
    "        A = linear_forward(A_prev, W, b)\n",
    "\n",
    "        # print(f\"{A=}, {A_prev=}\")\n",
    "        # print()\n",
    "    return A, A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a42e7c-d695-4238-9eb6-2a22b9ccfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, parameters):\n",
    "\n",
    "    #Forward pass\n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "\n",
    "    Z1 = linear_forward(X, W1, b1)  # (1,2)\n",
    "    A1 = Z1\n",
    "    Z2 = linear_forward(Z1, W2, b2) # (1,1)\n",
    "    y_hat = Z2\n",
    "\n",
    "    #Backward pass\n",
    "    dL_dyhat = -2 * (y - y_hat)  # (1,1)\n",
    "\n",
    "    dW2 = A1.T @ dL_dyhat # (2,1)\n",
    "    db2 = dL_dyhat # (1,1)\n",
    "\n",
    "    dA1 = dL_dyhat @ W2.T # (1, 2)\n",
    "    dZ1 = dA1 # (1, 2)\n",
    "    dW1 = X.T @ dZ1 # (2,4)\n",
    "    db1 = dZ1\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1,\n",
    "             \"dW2\": dW2, \"db2\": db2,\n",
    "             \"y_hat\": y_hat, \"A1\": A1}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55495c96-01bd-4da7-8021-c64f4df5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr):\n",
    "    parameters[\"W1\"] -= lr * grads[\"dW1\"]\n",
    "    parameters[\"b1\"] -= lr * grads[\"db1\"]\n",
    "    parameters[\"W2\"] -= lr * grads[\"dW2\"]\n",
    "    parameters[\"b2\"] -= lr * grads[\"db2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b990397-ae74-4eda-9359-c23510dd06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an older function that i made.\n",
    "# the better version is the one above, where we calculate the gradients separately and then update the parameters\n",
    "\n",
    "\n",
    "# def update_parameters(parameters, y, y_hat, lr, A1, X):\n",
    "#     scalar = -2 * (y - y_hat)\n",
    "\n",
    "#     parameters[\"W2\"] = parameters['W2'] - (lr * scalar * A1.T)\n",
    "#     parameters[\"b2\"] = parameters[\"b2\"] - (lr * scalar)\n",
    "\n",
    "#     parameters[\"W1\"] = parameters[\"W1\"] - (lr * scalar * np.dot(parameters[\"W2\"], X).T)\n",
    "#     parameters[\"b1\"] = parameters[\"b1\"] - (lr * scalar * parameters[\"W2\"].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33dc82fa-33ce-4f2d-a8f3-1acb434338ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  205.78238002173862\n",
      "Epoch -  2 Loss -  3.008717539293672\n",
      "Epoch -  3 Loss -  2.5970403350934363\n",
      "Epoch -  4 Loss -  2.2837504049340906\n",
      "Epoch -  5 Loss -  2.0403465856750644\n",
      "Epoch -  6 Loss -  1.8476338330742543\n",
      "Epoch -  7 Loss -  1.6921793236811848\n",
      "Epoch -  8 Loss -  1.564512443482407\n",
      "Epoch -  9 Loss -  1.4579071645975847\n",
      "Epoch -  10 Loss -  1.3675489102261333\n",
      "Epoch -  11 Loss -  1.2899600573297798\n",
      "Epoch -  12 Loss -  1.2226016844675744\n",
      "Epoch -  13 Loss -  1.163596487907929\n",
      "Epoch -  14 Loss -  1.1115355053058165\n",
      "Epoch -  15 Loss -  1.0653430437713438\n",
      "Epoch -  16 Loss -  1.024182141769514\n",
      "Epoch -  17 Loss -  0.9873883089586546\n",
      "Epoch -  18 Loss -  0.9544230144945101\n",
      "Epoch -  19 Loss -  0.9248409738555899\n",
      "Epoch -  20 Loss -  0.8982670773427317\n",
      "Epoch -  21 Loss -  0.8743800536201196\n",
      "Epoch -  22 Loss -  0.8529008352835575\n",
      "Epoch -  23 Loss -  0.8335842048586176\n",
      "Epoch -  24 Loss -  0.8162127279768306\n",
      "Epoch -  25 Loss -  0.8005922807193573\n",
      "Epoch -  26 Loss -  0.7865486885626976\n",
      "Epoch -  27 Loss -  0.773925141779441\n",
      "Epoch -  28 Loss -  0.7625801552880775\n",
      "Epoch -  29 Loss -  0.7523859129671076\n",
      "Epoch -  30 Loss -  0.7432268865974223\n",
      "Epoch -  31 Loss -  0.7349986543829524\n",
      "Epoch -  32 Loss -  0.7276068680088231\n",
      "Epoch -  33 Loss -  0.7209663336617049\n",
      "Epoch -  34 Loss -  0.7150001836383236\n",
      "Epoch -  35 Loss -  0.709639122712328\n",
      "Epoch -  36 Loss -  0.704820738447454\n",
      "Epoch -  37 Loss -  0.7004888679315624\n",
      "Epoch -  38 Loss -  0.6965930155179143\n",
      "Epoch -  39 Loss -  0.6930878174859777\n",
      "Epoch -  40 Loss -  0.6899325503424885\n",
      "Epoch -  41 Loss -  0.6870906799606767\n",
      "Epoch -  42 Loss -  0.6845294490265458\n",
      "Epoch -  43 Loss -  0.6822195004110015\n",
      "Epoch -  44 Loss -  0.6801345341704592\n",
      "Epoch -  45 Loss -  0.6782509959322998\n",
      "Epoch -  46 Loss -  0.6765477944661715\n",
      "Epoch -  47 Loss -  0.6750060462907996\n",
      "Epoch -  48 Loss -  0.673608845224473\n",
      "Epoch -  49 Loss -  0.6723410548580789\n",
      "Epoch -  50 Loss -  0.671189122012542\n"
     ]
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([4,2,1])\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(10):\n",
    "\n",
    "    X = data[j, :4].reshape(1,4) # Shape(no of features, no. of training example)\n",
    "    y = data[j, 4]\n",
    "\n",
    "    # Parameter initialization\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    grads = compute_gradients(X, y, parameters)\n",
    "    update_parameters(parameters, grads, lr)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  # print(\"\\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bb4fe-1b71-41c0-97dc-f97e75cf6a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20030cae-1e98-4d8f-aa01-3b6080cc85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example used in CampusX's backpropagation videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "25bcba1f-c7a8-4f13-8a1c-d185aceb2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4], [7,9,5], [6,10,6], [5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d21d65dd-7708-4827-acb0-ce8aadeb3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  7.903061447212958\n",
      "\n",
      "\n",
      "Epoch -  2 Loss -  3.190277370979962\n",
      "\n",
      "\n",
      "Epoch -  3 Loss -  1.3875770144953572\n",
      "\n",
      "\n",
      "Epoch -  4 Loss -  0.9535781917194645\n",
      "\n",
      "\n",
      "Epoch -  5 Loss -  0.8961373349576718\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([2,2,1])\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(4):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(1,2) # Shape(no of features, no. of training example)\n",
    "    y = df[['lpa']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    update_parameters(parameters, y, y_hat, 0.001, A1, X)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  print(\"\\n\")\n",
    "  \n",
    "\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c92e93-3f5e-4085-9ddb-137793e6318a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
