{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4acb196-a6f3-4e0a-8451-60bd3c83db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df4aa2e-830a-444a-972a-0ed87e2b2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([\n",
    "    [2, 5, 3, 7, 15],\n",
    "    [4, 6, 2, 8, 20],\n",
    "    [5, 7, 4, 6, 22],\n",
    "    [3, 8, 5, 5, 21],\n",
    "    [6, 5, 6, 7, 24],\n",
    "    [7, 3, 5, 8, 23],\n",
    "    [8, 4, 7, 6, 25],\n",
    "    [5, 6, 4, 9, 24],\n",
    "    [6, 7, 3, 5, 21],\n",
    "    [4, 5, 6, 7, 22]\n",
    "], columns=['X1', 'X2', 'X3', 'X4', 'Y'])\n",
    "\n",
    "data = data.to_numpy()\n",
    "X = data[:, :4].T\n",
    "y = data[:, [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab308e5-575d-4a7a-be5b-dce3393f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for layer in range(1, L):\n",
    "        # the cols are weights associated with a node of current layer\n",
    "        # the rows are weights assoc with the node of previous layer\n",
    "        parameters[\"W\" + str(layer)] = np.random.rand(layer_dims[layer-1], layer_dims[layer]) - .5\n",
    "\n",
    "        parameters[\"b\" + str(layer)] = np.random.rand(1, layer_dims[layer]) -.5\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b533087-a13a-4db0-af8e-44c91fba28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    return np.dot(A_prev, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab70ae4-9cee-4976-a7bb-2744d30c2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for layer in range(1, L+1):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(layer)]\n",
    "        b = parameters['b' + str(layer)]\n",
    "        \n",
    "        A = linear_forward(A_prev, W, b)\n",
    "\n",
    "        # print(f\"{A=}, {A_prev=}\")\n",
    "        # print()\n",
    "    return A, A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a42e7c-d695-4238-9eb6-2a22b9ccfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, parameters):\n",
    "\n",
    "    #Forward pass\n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "\n",
    "    Z1 = linear_forward(X, W1, b1)  # (1,2)\n",
    "    A1 = Z1\n",
    "    Z2 = linear_forward(Z1, W2, b2) # (1,1)\n",
    "    y_hat = Z2\n",
    "\n",
    "    #Backward pass\n",
    "    dL_dyhat = -2 * (y - y_hat)  # (1,1)\n",
    "\n",
    "    dW2 = A1.T @ dL_dyhat # (2,1)\n",
    "    db2 = dL_dyhat # (1,1)\n",
    "\n",
    "    dA1 = dL_dyhat @ W2.T # (1, 2)\n",
    "    dZ1 = dA1 # (1, 2)\n",
    "    dW1 = X.T @ dZ1 # (2,4)\n",
    "    db1 = dZ1\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1,\n",
    "             \"dW2\": dW2, \"db2\": db2,\n",
    "             \"y_hat\": y_hat, \"A1\": A1}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55495c96-01bd-4da7-8021-c64f4df5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr):\n",
    "    parameters[\"W1\"] -= lr * grads[\"dW1\"]\n",
    "    parameters[\"b1\"] -= lr * grads[\"db1\"]\n",
    "    parameters[\"W2\"] -= lr * grads[\"dW2\"]\n",
    "    parameters[\"b2\"] -= lr * grads[\"db2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b990397-ae74-4eda-9359-c23510dd06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an older function that i made.\n",
    "# the better version is the one above, where we calculate the gradients separately and then update the parameters\n",
    "\n",
    "\n",
    "# def update_parameters(parameters, y, y_hat, lr, A1, X):\n",
    "#     scalar = -2 * (y - y_hat)\n",
    "\n",
    "#     parameters[\"W2\"] = parameters['W2'] - (lr * scalar * A1.T)\n",
    "#     parameters[\"b2\"] = parameters[\"b2\"] - (lr * scalar)\n",
    "\n",
    "#     parameters[\"W1\"] = parameters[\"W1\"] - (lr * scalar * np.dot(parameters[\"W2\"], X).T)\n",
    "#     parameters[\"b1\"] = parameters[\"b1\"] - (lr * scalar * parameters[\"W2\"].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33dc82fa-33ce-4f2d-a8f3-1acb434338ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  152.30652470205393\n",
      "Epoch -  2 Loss -  4.631570438925524\n",
      "Epoch -  3 Loss -  3.772896759808063\n",
      "Epoch -  4 Loss -  3.1464173308563517\n",
      "Epoch -  5 Loss -  2.681344401804283\n",
      "Epoch -  6 Loss -  2.329723367155441\n",
      "Epoch -  7 Loss -  2.0588553350084364\n",
      "Epoch -  8 Loss -  1.8461779315090912\n",
      "Epoch -  9 Loss -  1.6759818260720922\n",
      "Epoch -  10 Loss -  1.5372454743549073\n",
      "Epoch -  11 Loss -  1.422180041488296\n",
      "Epoch -  12 Loss -  1.3252389955801087\n",
      "Epoch -  13 Loss -  1.2424381273101877\n",
      "Epoch -  14 Loss -  1.1708859499024409\n",
      "Epoch -  15 Loss -  1.108458086859238\n",
      "Epoch -  16 Loss -  1.053570872402748\n",
      "Epoch -  17 Loss -  1.005023626738432\n",
      "Epoch -  18 Loss -  0.9618886142068777\n",
      "Epoch -  19 Loss -  0.9234341748028356\n",
      "Epoch -  20 Loss -  0.8890709614945645\n",
      "Epoch -  21 Loss -  0.8583142792777823\n",
      "Epoch -  22 Loss -  0.8307576446471657\n",
      "Epoch -  23 Loss -  0.8060541601271337\n",
      "Epoch -  24 Loss -  0.7839033273447338\n",
      "Epoch -  25 Loss -  0.7640416405992184\n",
      "Epoch -  26 Loss -  0.7462358052139105\n",
      "Epoch -  27 Loss -  0.7302777763847847\n",
      "Epoch -  28 Loss -  0.7159810601016507\n",
      "Epoch -  29 Loss -  0.703177889615607\n",
      "Epoch -  30 Loss -  0.6917170109475625\n",
      "Epoch -  31 Loss -  0.6814618945492609\n",
      "Epoch -  32 Loss -  0.6722892482976996\n",
      "Epoch -  33 Loss -  0.664087747157721\n",
      "Epoch -  34 Loss -  0.6567569224558107\n",
      "Epoch -  35 Loss -  0.6502061725517717\n",
      "Epoch -  36 Loss -  0.6443538694369015\n",
      "Epoch -  37 Loss -  0.6391265443023102\n",
      "Epoch -  38 Loss -  0.6344581407264354\n",
      "Epoch -  39 Loss -  0.6302893277514565\n",
      "Epoch -  40 Loss -  0.6265668674007048\n",
      "Epoch -  41 Loss -  0.6232430325820759\n",
      "Epoch -  42 Loss -  0.6202750721380569\n",
      "Epoch -  43 Loss -  0.6176247202527064\n",
      "Epoch -  44 Loss -  0.6152577476551976\n",
      "Epoch -  45 Loss -  0.6131435521627123\n",
      "Epoch -  46 Loss -  0.6112547861462392\n",
      "Epoch -  47 Loss -  0.6095670185191465\n",
      "Epoch -  48 Loss -  0.6080584288644653\n",
      "Epoch -  49 Loss -  0.6067095313448354\n",
      "Epoch -  50 Loss -  0.605502926085176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-8.44730634e-01, -1.68622266e-01],\n",
       "        [-7.57253081e-01, -1.43731854e-01],\n",
       "        [-5.99810557e-01, -4.01286838e-01],\n",
       "        [-7.16040345e-01,  8.14166499e-04]]),\n",
       " 'b1': array([[ 0.09952364, -0.21972073]]),\n",
       " 'W2': array([[-1.31461726],\n",
       "        [-0.33868701]]),\n",
       " 'b2': array([[0.00672781]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([4,2,1])\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(10):\n",
    "\n",
    "    X = data[j, :4].reshape(1,4) # Shape(no of features, no. of training example)\n",
    "    y = data[j, 4]\n",
    "\n",
    "    # Parameter initialization\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    grads = compute_gradients(X, y, parameters)\n",
    "    update_parameters(parameters, grads, lr)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  # print(\"\\n\")\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bb4fe-1b71-41c0-97dc-f97e75cf6a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20030cae-1e98-4d8f-aa01-3b6080cc85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example used in CampusX's backpropagation videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25bcba1f-c7a8-4f13-8a1c-d185aceb2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4], [7,9,5], [6,10,6], [5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21d65dd-7708-4827-acb0-ce8aadeb3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  27.08286100691636\n",
      "Epoch -  2 Loss -  10.858235118624997\n",
      "Epoch -  3 Loss -  2.941541078583097\n",
      "Epoch -  4 Loss -  1.0261381881056533\n",
      "Epoch -  5 Loss -  0.8975238641227243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.3981836 , 0.21162732],\n",
       "        [0.10289689, 0.6800775 ]]),\n",
       " 'b1': array([[-0.3591457 , -0.03739591]]),\n",
       " 'W2': array([[0.41611166],\n",
       "        [0.53862126]]),\n",
       " 'b2': array([[0.11536361]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([2,2,1])\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(4):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(1,2) # Shape(no of features, no. of training example)\n",
    "    y = df[['lpa']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    grads = compute_gradients(X, y, parameters)\n",
    "    update_parameters(parameters, grads, lr)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  # print(\"\\n\")\n",
    "  \n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c92e93-3f5e-4085-9ddb-137793e6318a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
