{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4acb196-a6f3-4e0a-8451-60bd3c83db2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df4aa2e-830a-444a-972a-0ed87e2b2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([\n",
    "    [2, 5, 3, 7, 15],\n",
    "    [4, 6, 2, 8, 20],\n",
    "    [5, 7, 4, 6, 22],\n",
    "    [3, 8, 5, 5, 21],\n",
    "    [6, 5, 6, 7, 24],\n",
    "    [7, 3, 5, 8, 23],\n",
    "    [8, 4, 7, 6, 25],\n",
    "    [5, 6, 4, 9, 24],\n",
    "    [6, 7, 3, 5, 21],\n",
    "    [4, 5, 6, 7, 22]\n",
    "], columns=['X1', 'X2', 'X3', 'X4', 'Y'])\n",
    "\n",
    "data = data.to_numpy()\n",
    "X = data[:, :4].T\n",
    "y = data[:, [4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ab308e5-575d-4a7a-be5b-dce3393f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for layer in range(1, L):\n",
    "        # the cols are weights associated with a node of current layer \n",
    "        # the rows are weights assoc with the node of previous layer\n",
    "        parameters[\"W\" + str(layer)] = np.random.rand(layer_dims[layer-1], layer_dims[layer]) - .5\n",
    "\n",
    "        parameters[\"b\" + str(layer)] = np.random.rand(1, layer_dims[layer]) -.5\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b533087-a13a-4db0-af8e-44c91fba28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A_prev, W, b):\n",
    "    return np.dot(A_prev, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab70ae4-9cee-4976-a7bb-2744d30c2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for layer in range(1, L+1):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(layer)]\n",
    "        b = parameters['b' + str(layer)]\n",
    "        \n",
    "        A = linear_forward(A_prev, W, b)\n",
    "\n",
    "        # print(f\"{A=}, {A_prev=}\")\n",
    "        # print()\n",
    "    return A, A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a42e7c-d695-4238-9eb6-2a22b9ccfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, parameters):\n",
    "\n",
    "    #Forward pass\n",
    "    W1, b1 = parameters[\"W1\"], parameters[\"b1\"]\n",
    "    W2, b2 = parameters[\"W2\"], parameters[\"b2\"]\n",
    "\n",
    "    Z1 = linear_forward(X, W1, b1)  # (1,2)\n",
    "    A1 = Z1\n",
    "    Z2 = linear_forward(Z1, W2, b2) # (1,1)\n",
    "    y_hat = Z2\n",
    "\n",
    "    #Backward pass\n",
    "    dL_dyhat = -2 * (y - y_hat)  # (1,1)\n",
    "\n",
    "    dW2 = A1.T @ dL_dyhat # (2,1)\n",
    "    db2 = dL_dyhat # (1,1)\n",
    "\n",
    "    dA1 = dL_dyhat @ W2.T # (1, 2)\n",
    "    dZ1 = dA1 # (1, 2)\n",
    "    dW1 = X.T @ dZ1 # (2,4)\n",
    "    db1 = dZ1\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1,\n",
    "             \"dW2\": dW2, \"db2\": db2,\n",
    "             \"y_hat\": y_hat, \"A1\": A1}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55495c96-01bd-4da7-8021-c64f4df5403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, lr):\n",
    "    parameters[\"W1\"] -= lr * grads[\"dW1\"]\n",
    "    parameters[\"b1\"] -= lr * grads[\"db1\"]\n",
    "    parameters[\"W2\"] -= lr * grads[\"dW2\"]\n",
    "    parameters[\"b2\"] -= lr * grads[\"db2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b990397-ae74-4eda-9359-c23510dd06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an older function that i made.\n",
    "# the better version is the one above, where we calculate the gradients separately and then update the parameters\n",
    "\n",
    "\n",
    "# def update_parameters(parameters, y, y_hat, lr, A1, X):\n",
    "#     scalar = -2 * (y - y_hat)\n",
    "\n",
    "#     parameters[\"W2\"] = parameters['W2'] - (lr * scalar * A1.T)\n",
    "#     parameters[\"b2\"] = parameters[\"b2\"] - (lr * scalar)\n",
    "\n",
    "#     parameters[\"W1\"] = parameters[\"W1\"] - (lr * scalar * np.dot(parameters[\"W2\"], X).T)\n",
    "#     parameters[\"b1\"] = parameters[\"b1\"] - (lr * scalar * parameters[\"W2\"].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33dc82fa-33ce-4f2d-a8f3-1acb434338ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  195.5232299027706\n",
      "Epoch -  2 Loss -  1.7279994044502374\n",
      "Epoch -  3 Loss -  1.5311742139138413\n",
      "Epoch -  4 Loss -  1.3766854832695383\n",
      "Epoch -  5 Loss -  1.254353000906228\n",
      "Epoch -  6 Loss -  1.1567605831349803\n",
      "Epoch -  7 Loss -  1.078381057815306\n",
      "Epoch -  8 Loss -  1.0150425424016134\n",
      "Epoch -  9 Loss -  0.9635623548199582\n",
      "Epoch -  10 Loss -  0.9214902930025396\n",
      "Epoch -  11 Loss -  0.8869251921198991\n",
      "Epoch -  12 Loss -  0.8583817223825013\n",
      "Epoch -  13 Loss -  0.8346923288807646\n",
      "Epoch -  14 Loss -  0.8149341840218088\n",
      "Epoch -  15 Loss -  0.7983742147458802\n",
      "Epoch -  16 Loss -  0.7844273647184965\n",
      "Epoch -  17 Loss -  0.7726246599044693\n",
      "Epoch -  18 Loss -  0.7625886090919\n",
      "Epoch -  19 Loss -  0.7540141409783255\n",
      "Epoch -  20 Loss -  0.7466537527436771\n",
      "Epoch -  21 Loss -  0.7403058840085427\n",
      "Epoch -  22 Loss -  0.7348057758554343\n",
      "Epoch -  23 Loss -  0.7300182547777154\n",
      "Epoch -  24 Loss -  0.7258320148312125\n",
      "Epoch -  25 Loss -  0.7221550709138947\n",
      "Epoch -  26 Loss -  0.7189111311182612\n",
      "Epoch -  27 Loss -  0.7160366929722355\n",
      "Epoch -  28 Loss -  0.713478711766695\n",
      "Epoch -  29 Loss -  0.7111927224430763\n",
      "Epoch -  30 Loss -  0.7091413221643059\n",
      "Epoch -  31 Loss -  0.7072929405512245\n",
      "Epoch -  32 Loss -  0.7056208400033646\n",
      "Epoch -  33 Loss -  0.7041023005650571\n",
      "Epoch -  34 Loss -  0.7027179532231215\n",
      "Epoch -  35 Loss -  0.7014512329210052\n",
      "Epoch -  36 Loss -  0.7002879283987122\n",
      "Epoch -  37 Loss -  0.6992158105648251\n",
      "Epoch -  38 Loss -  0.6982243247439688\n",
      "Epoch -  39 Loss -  0.6973043350278364\n",
      "Epoch -  40 Loss -  0.6964479112508167\n",
      "Epoch -  41 Loss -  0.6956481509380905\n",
      "Epoch -  42 Loss -  0.6948990300329911\n",
      "Epoch -  43 Loss -  0.6941952773777484\n",
      "Epoch -  44 Loss -  0.6935322688580541\n",
      "Epoch -  45 Loss -  0.6929059378749496\n",
      "Epoch -  46 Loss -  0.6923126994138099\n",
      "Epoch -  47 Loss -  0.6917493854705349\n",
      "Epoch -  48 Loss -  0.6912131899916101\n",
      "Epoch -  49 Loss -  0.6907016218069079\n",
      "Epoch -  50 Loss -  0.690212464296156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.20104244,  0.82975883],\n",
       "        [ 0.12214386,  0.77455879],\n",
       "        [ 0.15281256,  0.72235402],\n",
       "        [-0.59299841,  0.52750985]]),\n",
       " 'b1': array([[-0.16588189,  0.38340484]]),\n",
       " 'W2': array([[-0.36323589],\n",
       "        [ 1.31060347]]),\n",
       " 'b2': array([[0.21533049]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([4,2,1])\n",
    "epochs = 50\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(10):\n",
    "\n",
    "    X = data[j, :4].reshape(1,4) # Shape(no of features, no. of training example)\n",
    "    y = data[j, 4]\n",
    "\n",
    "    # Parameter initialization\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    grads = compute_gradients(X, y, parameters)\n",
    "    update_parameters(parameters, grads, lr)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  # print(\"\\n\")\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bb4fe-1b71-41c0-97dc-f97e75cf6a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20030cae-1e98-4d8f-aa01-3b6080cc85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example used in CampusX's backpropagation videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25bcba1f-c7a8-4f13-8a1c-d185aceb2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[8,8,4], [7,9,5], [6,10,6], [5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21d65dd-7708-4827-acb0-ce8aadeb3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -  1 Loss -  27.08286100691636\n",
      "Epoch -  2 Loss -  10.858235118624997\n",
      "Epoch -  3 Loss -  2.941541078583097\n",
      "Epoch -  4 Loss -  1.0261381881056533\n",
      "Epoch -  5 Loss -  0.8975238641227243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.3981836 , 0.21162732],\n",
       "        [0.10289689, 0.6800775 ]]),\n",
       " 'b1': array([[-0.3591457 , -0.03739591]]),\n",
       " 'W2': array([[0.41611166],\n",
       "        [0.53862126]]),\n",
       " 'b2': array([[0.11536361]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs implementation\n",
    "\n",
    "parameters = init_parameters([2,2,1])\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  Loss = []\n",
    "\n",
    "  for j in range(4):\n",
    "\n",
    "    X = df[['cgpa', 'profile_score']].values[j].reshape(1,2) # Shape(no of features, no. of training example)\n",
    "    y = df[['lpa']].values[j][0]\n",
    "\n",
    "    # Parameter initialization\n",
    "\n",
    "\n",
    "    y_hat,A1 = forward_prop(X,parameters)\n",
    "    y_hat = y_hat[0][0]\n",
    "\n",
    "    grads = compute_gradients(X, y, parameters)\n",
    "    update_parameters(parameters, grads, lr)\n",
    "\n",
    "    Loss.append((y-y_hat)**2)\n",
    "  print('Epoch - ',i+1,'Loss - ',np.array(Loss).mean())\n",
    "  # print(\"\\n\")\n",
    "  \n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c92e93-3f5e-4085-9ddb-137793e6318a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
